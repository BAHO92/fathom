#!/usr/bin/env python3
"""
ì‹¤ë¡ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ (sillok_crawler.py)
ì¡°ì„ ì™•ì¡°ì‹¤ë¡ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ JSONìœ¼ë¡œ ì €ì¥

Usage:
    python sillok_crawler.py <input.txt> --name <collection_name>

Examples:
    python sillok_crawler.py ~/Downloads/ì‹¤ë¡_íš¨ì¢…ì‹¤ë¡.txt --name ì†¡ì‹œì—´_íš¨ì¢…ì‹¤ë¡
    python sillok_crawler.py urls.txt --name ì‚°ë¦¼_ìˆ™ì¢…ì‹¤ë¡ --workers 6

Input file format (TSV with ^ delimiter, from ì‹¤ë¡ ì‚¬ì´íŠ¸ ê²€ìƒ‰ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ):
    ê¸°ì‚¬ ID^ê¶Œìˆ˜^ë‚ ì§œ^ì œëª©^ê¸°ì‚¬ URL

Output:
    DB/Sillok/{collection_name}/
        â”œâ”€â”€ raw/              # ê¸°ì‚¬ë³„ ê°œë³„ JSON
        â”œâ”€â”€ metadata.json     # ìˆ˜ì§‘ ë©”íƒ€ì •ë³´
        â””â”€â”€ failed_articles.json  # ì‹¤íŒ¨ ë¡œê·¸ (ì‹¤íŒ¨ ì‹œ)
"""

import argparse
import json
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
def check_and_install_packages():
    required = {
        'requests': 'requests',
        'bs4': 'beautifulsoup4'
    }
    for pkg, pip_name in required.items():
        try:
            __import__(pkg)
        except ImportError:
            print(f"Installing {pip_name}...")
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", pip_name])

check_and_install_packages()

import requests
from bs4 import BeautifulSoup

# ê¸€ë¡œë²Œ ë³€ìˆ˜
print_lock = Lock()
progress_lock = Lock()
processed_count = 0
total_count = 0

# DB ê²½ë¡œ
def _get_db_root() -> Path:
    import os
    env = os.environ.get("KHC_DB_ROOT")
    if env:
        return Path(env).expanduser().resolve()
    here = Path(__file__).resolve().parent
    for base in (here, *here.parents):
        cf = base / "crawl-orchestrator" / "config.json"
        if cf.is_file():
            try:
                c = json.loads(cf.read_text(encoding="utf-8"))
                raw = c.get("db_root", "")
                if raw:
                    p = Path(raw).expanduser()
                    return (p if p.is_absolute() else cf.parent / p).resolve()
            except Exception:
                pass
            return (base / "DB").resolve()
    return Path.home() / "DB"

DB_PATH = _get_db_root() / "Sillok"

# HTTP ì„¸ì…˜ ì„¤ì •
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"


def safe_print(*args, **kwargs):
    """ìŠ¤ë ˆë“œ ì•ˆì „ ì¶œë ¥"""
    with print_lock:
        print(*args, **kwargs, flush=True)


def create_session() -> requests.Session:
    """HTTP ì„¸ì…˜ ìƒì„±"""
    session = requests.Session()
    session.headers.update({
        'User-Agent': USER_AGENT,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    })
    return session


def parse_input_file(filepath: str, input_format: str = 'auto') -> list[dict]:
    """
    ì…ë ¥ íŒŒì¼ íŒŒì‹± (TSV ë˜ëŠ” JSON)

    Args:
        filepath: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
        input_format: 'auto', 'tsv', 'json' ì¤‘ í•˜ë‚˜

    Returns:
        ê¸°ì‚¬ ëª©ë¡
    """
    # í¬ë§· ìë™ ê°ì§€
    if input_format == 'auto':
        if filepath.endswith('.json'):
            input_format = 'json'
        else:
            input_format = 'tsv'

    if input_format == 'json':
        return parse_json_input(filepath)
    else:
        return parse_tsv_input(filepath)


def parse_json_input(filepath: str) -> list[dict]:
    """JSON ì…ë ¥ íŒŒì¼ íŒŒì‹± (sillok_search.py ì¶œë ¥ í˜•ì‹)"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # sillok_search.py ì¶œë ¥ í˜•ì‹: {'metadata': {...}, 'entries': [...]}
    if 'entries' in data:
        raw_entries = data['entries']
    elif isinstance(data, list):
        raw_entries = data
    else:
        raw_entries = []

    entries = []
    for item in raw_entries:
        # ë‚ ì§œ ì •ë³´ íŒŒì‹±
        date_str = item.get('date', '')
        date_info = parse_date_info(date_str)

        entry = {
            'id': item.get('id', ''),
            'volume_info': item.get('volume', ''),
            'date_str': date_str,
            'date': date_info,
            'title': item.get('title', ''),
            'url': item.get('url', '')
        }
        entries.append(entry)

    return entries


def parse_tsv_input(filepath: str) -> list[dict]:
    """TSV ì…ë ¥ íŒŒì¼ íŒŒì‹± (^ êµ¬ë¶„ì)"""
    entries = []
    with open(filepath, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # í—¤ë” ìŠ¤í‚µ
    for line in lines[1:]:
        line = line.strip()
        if not line:
            continue

        parts = line.split('^')
        if len(parts) >= 5:
            # ë‚ ì§œ ì •ë³´ íŒŒì‹±
            date_info = parse_date_info(parts[2])

            entry = {
                'id': parts[0],
                'volume_info': parts[1],  # ì˜ˆ: "íš¨ì¢…ì‹¤ë¡ 5ê¶Œ"
                'date_str': parts[2],     # ì›ë³¸ ë‚ ì§œ ë¬¸ìì—´
                'date': date_info,
                'title': parts[3],
                'url': parts[4]
            }
            entries.append(entry)

    return entries


def parse_date_info(date_str: str) -> dict:
    """
    ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±

    Examples:
        "íš¨ì¢… 1ë…„ 2ì›” 3ì¼ ç”²ç”³ 1ë²ˆì§¸ê¸°ì‚¬" â†’ ì¤‘êµ­ì–´ ê°„ì§€, Në²ˆì§¸ í˜•ì‹
        "í˜„ì¢… ì¦‰ìœ„ë…„ 5ì›” 4ì¼ ê°‘ì 2/7 ê¸°ì‚¬" â†’ í•œêµ­ì–´ ê°„ì§€, N/M í˜•ì‹
    """
    result = {
        'reign': '',
        'year': 0,
        'month': 0,
        'day': 0,
        'ganzhi': '',
        'article_num': 1
    }

    # ì™•ëŒ€ ì¶”ì¶œ
    reign_match = re.match(r'^(\S+)\s+', date_str)
    if reign_match:
        result['reign'] = reign_match.group(1)

    # ì—°ì›”ì¼ ì¶”ì¶œ (ì¦‰ìœ„ë…„ = 0ë…„)
    year_match = re.search(r'(\d+)ë…„', date_str)
    month_match = re.search(r'(\d+)ì›”', date_str)
    day_match = re.search(r'(\d+)ì¼', date_str)

    if year_match:
        result['year'] = int(year_match.group(1))
    # ì¦‰ìœ„ë…„ì€ year=0ìœ¼ë¡œ ìœ ì§€ (ê¸°ë³¸ê°’)
    if month_match:
        result['month'] = int(month_match.group(1))
    if day_match:
        result['day'] = int(day_match.group(1))

    # ê°„ì§€ ì¶”ì¶œ (ì¤‘êµ­ì–´ + í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
    ganzhi_pattern = r'[ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸ê°‘ì„ë³‘ì •ë¬´ê¸°ê²½ì‹ ì„ê³„][å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥ìì¶•ì¸ë¬˜ì§„ì‚¬ì˜¤ë¯¸ì‹ ìœ ìˆ í•´]'
    ganzhi_match = re.search(ganzhi_pattern, date_str)
    if ganzhi_match:
        result['ganzhi'] = ganzhi_match.group(0)

    # ê¸°ì‚¬ ë²ˆí˜¸ ì¶”ì¶œ
    num_match = re.search(r'(\d+)ë²ˆì§¸', date_str)
    if num_match:
        result['article_num'] = int(num_match.group(1))
    else:
        fraction_match = re.search(r'(\d+)/\d+\s*ê¸°ì‚¬', date_str)
        if fraction_match:
            result['article_num'] = int(fraction_match.group(1))

    return result


def parse_volume_info(volume_str: str) -> dict:
    """
    ê¶Œìˆ˜ ì •ë³´ íŒŒì‹±
    ì˜ˆ: "íš¨ì¢…ì‹¤ë¡ 5ê¶Œ" â†’ {"sillok": "íš¨ì¢…ì‹¤ë¡", "volume": 5}
    """
    result = {'sillok': '', 'volume': 0, 'page': ''}

    match = re.match(r'^(.+ì‹¤ë¡|.+ë³´ê¶ì •ì˜¤)\s*(\d+)ê¶Œ?', volume_str)
    if match:
        result['sillok'] = match.group(1).strip()
        result['volume'] = int(match.group(2))
    else:
        result['sillok'] = volume_str

    return result


def extract_footnotes(soup: BeautifulSoup) -> dict:
    """
    ul.ins_footnoteì—ì„œ ì£¼ì„ ì¶”ì¶œ

    Returns:
        {"017": {"term": "ë™ê¸°(åŒæ°£)", "definition": "ì†¡ì‹œì—´ì˜ í˜•ì„ ë§í•¨."}, ...}
    """
    footnotes = {}
    footnote_list = soup.select_one('ul.ins_footnote')

    if not footnote_list:
        return footnotes

    for item in footnote_list.select('li'):
        link = item.select_one('a')
        if not link:
            continue

        link_text = link.get_text(strip=True)
        marker_match = re.search(r'\[è¨»\s*(\d+)\]', link_text)

        if marker_match:
            marker = marker_match.group(1)
            item_text = item.get_text(strip=True)
            content = re.sub(r'\[è¨»\s*\d+\]\s*', '', item_text)

            if ' : ' in content:
                term, definition = content.split(' : ', 1)
            else:
                term, definition = "", content

            footnotes[marker] = {
                'term': term.strip(),
                'definition': definition.strip()
            }

    return footnotes


def fetch_article(session: requests.Session, url: str, retry: int = 3) -> dict:
    """
    ì‹¤ë¡ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë²ˆì—­ë¬¸, ì›ë¬¸, ì£¼ì„, ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (requests ë²„ì „)
    """
    result = {
        'translation': '',
        'original': '',
        'footnotes': {},
        'category': [],
        'page_info': '',
        'title': '',
        'date_info': None
    }

    for attempt in range(retry):
        try:
            response = session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # <p class="date"> ìš”ì†Œì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            date_elem = soup.select_one('p.date')
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                result['date_info'] = parse_date_info(date_text)

            # ì œëª© ì¶”ì¶œ â€” try known selectors first, then bare <h3> as
            # fallback for IDs-mode article pages where class-based
            # selectors don't match.
            for selector in ['h3.view-tit', '.tit_loc', 'h3.page_tit']:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if title_text:
                        result['title'] = title_text
                        break
            else:
                # IDs-mode fallback: bare <h3> without class, skip
                # known structural headings (content-title, etc.)
                skip_classes = {'content-title'}
                for h3 in soup.select('h3'):
                    h3_classes = set(h3.get('class') or [])
                    if h3_classes & skip_classes:
                        continue
                    if h3_classes:
                        continue
                    h3_text = h3.get_text(strip=True)
                    if h3_text:
                        result['title'] = h3_text
                        break

            # êµ­ì—­/ì›ë¬¸ ì¶”ì¶œ
            for h4 in soup.select('h4.view-title'):
                heading_text = h4.get_text(strip=True)

                if heading_text not in ['êµ­ì—­', 'ì›ë¬¸']:
                    continue

                parent_div = h4.parent
                if not parent_div:
                    continue

                view_text = parent_div.select_one('div.view-text')
                if not view_text:
                    continue

                paragraphs = view_text.select('p.paragraph')
                texts = []
                for p in paragraphs:
                    p_text = p.get_text(strip=True)
                    if p_text:
                        texts.append(p_text)

                combined_text = '\n\n'.join(texts)

                if heading_text == 'êµ­ì—­':
                    result['translation'] = combined_text
                elif heading_text == 'ì›ë¬¸':
                    result['original'] = combined_text

            # ì£¼ì„ ì¶”ì¶œ
            result['footnotes'] = extract_footnotes(soup)

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ã€ë¶„ë¥˜ã€‘)
            page_source = response.text
            category_section = re.search(r'ã€ë¶„ë¥˜ã€‘(.+?)(?=ã€|<div|$)', page_source, re.DOTALL)
            if category_section:
                cat_text = category_section.group(1)
                cat_matches = re.findall(r'\[([^\]]+)\]', cat_text)
                for cat in cat_matches:
                    clean_cat = re.sub(r'<[^>]+>', '', cat).strip()
                    if clean_cat and clean_cat not in result['category']:
                        result['category'].append(clean_cat)

            # ì¶œì „ ì •ë³´ ì¶”ì¶œ
            page_info_parts = []
            taebaek_match = re.search(r'ã€íƒœë°±ì‚°ì‚¬ê³ ë³¸ã€‘\s*([^ã€\n]+)', page_source)
            if taebaek_match:
                info = re.sub(r'<[^>]+>', '', taebaek_match.group(1)).strip()
                if info:
                    page_info_parts.append(f"íƒœë°±ì‚°ì‚¬ê³ ë³¸ {info}")

            gukpyeon_match = re.search(r'ã€êµ­í¸ì˜ì¸ë³¸ã€‘\s*([^ã€\n]+)', page_source)
            if gukpyeon_match:
                info = re.sub(r'<[^>]+>', '', gukpyeon_match.group(1)).strip()
                if info:
                    page_info_parts.append(f"êµ­í¸ì˜ì¸ë³¸ {info}")

            if page_info_parts:
                result['page_info'] = ' / '.join(page_info_parts)

            return result

        except Exception as e:
            if attempt < retry - 1:
                time.sleep(1)
            else:
                return {
                    'translation': '[í¬ë¡¤ë§ ì‹¤íŒ¨]',
                    'original': '',
                    'footnotes': {},
                    'category': [],
                    'page_info': '',
                    'title': '',
                    'date_info': None,
                    'error': str(e)
                }

    return result


def fetch_article_task(task: dict) -> dict:
    """ì›Œì»¤ìš©: ë‹¨ì¼ ê¸°ì‚¬ í¬ë¡¤ë§"""
    global processed_count, total_count

    session = create_session()

    try:
        url = task['url']
        content = fetch_article(session, url)

        with progress_lock:
            processed_count += 1
            current = processed_count

        title_preview = task['title'][:40] if task['title'] else 'No title'
        safe_print(f"[{current}/{total_count}] Done: {title_preview}...")

        # ê²°ê³¼ ì¡°í•©
        source = parse_volume_info(task['volume_info'])

        # ë‚ ì§œ ì •ë³´ ë³‘í•©
        date_info = task['date'].copy()
        if content.get('date_info'):
            page_date = content['date_info']
            if page_date.get('ganzhi'):
                date_info['ganzhi'] = page_date['ganzhi']
            if page_date.get('article_num'):
                date_info['article_num'] = page_date['article_num']

        article = {
            'id': task['id'],
            'title': content['title'] or task['title'],
            'date': date_info,
            'article_num': date_info['article_num'],
            'source': source,
            'url': task['url'],
            'original': content['original'],
            'translation': content['translation'],
            'has_translation': bool(content['translation'].strip()),
            'source_specific': {
                'footnotes': content['footnotes'],
                'page_info': content['page_info'],
                'category': content['category']
            }
        }

        # í¬ë¡¤ë§ ì‹¤íŒ¨ ì²´í¬
        if content.get('error') or content['translation'] == '[í¬ë¡¤ë§ ì‹¤íŒ¨]':
            return {
                'article': None,
                'success': False,
                'error': {
                    'id': task['id'],
                    'url': task['url'],
                    'title': task['title'],
                    'error_type': 'FetchError',
                    'error_message': content.get('error', 'Unknown error'),
                    'timestamp': datetime.now().isoformat()
                }
            }

        return {
            'article': article,
            'success': True
        }

    except Exception as e:
        safe_print(f"Error fetching {task.get('url', 'unknown')}: {e}")
        return {
            'article': None,
            'success': False,
            'error': {
                'id': task.get('id', 'unknown'),
                'url': task.get('url', 'unknown'),
                'title': task.get('title', 'unknown'),
                'error_type': type(e).__name__,
                'error_message': str(e),
                'timestamp': datetime.now().isoformat()
            }
        }


def crawl_articles(entries: list[dict], collection_name: str, num_workers: int = 4, resume: bool = False):
    """ê¸°ì‚¬ë“¤ í¬ë¡¤ë§"""
    global processed_count, total_count

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = DB_PATH / collection_name
    raw_dir = output_dir / "raw"
    output_dir.mkdir(parents=True, exist_ok=True)
    raw_dir.mkdir(exist_ok=True)

    # ì§„í–‰ íŒŒì¼
    progress_file = output_dir / ".progress.json"

    # ì¬ê°œ ëª¨ë“œ
    completed_ids = set()
    if resume:
        if progress_file.exists():
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
                completed_ids = set(progress_data.get('completed_ids', []))
        for json_file in raw_dir.glob("*.json"):
            completed_ids.add(json_file.stem)
        if completed_ids:
            print(f"Resuming: {len(completed_ids)} entries already completed", flush=True)

    # ë‚¨ì€ ì‘ì—… í•„í„°ë§
    remaining = [e for e in entries if e['id'] not in completed_ids]
    total_count = len(remaining)
    processed_count = 0

    print(f"\nProcessing {len(remaining)} entries with {num_workers} workers...", flush=True)

    # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
    articles = []
    failed_articles = []
    if resume:
        for json_file in raw_dir.glob("*.json"):
            with open(json_file, 'r', encoding='utf-8') as f:
                articles.append(json.load(f))

    # ë³‘ë ¬ í¬ë¡¤ë§
    if remaining:
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = {executor.submit(fetch_article_task, e): e for e in remaining}

            for future in as_completed(futures):
                result = future.result()
                if result['success'] and result['article']:
                    article = result['article']
                    articles.append(article)
                    completed_ids.add(article['id'])

                    # ê°œë³„ JSON ì €ì¥
                    article_file = raw_dir / f"{article['id']}.json"
                    with open(article_file, 'w', encoding='utf-8') as f:
                        json.dump(article, f, ensure_ascii=False, indent=2)

                    # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì €ì¥
                    if len(completed_ids) % 10 == 0:
                        with open(progress_file, 'w', encoding='utf-8') as f:
                            json.dump({'completed_ids': list(completed_ids)}, f)
                else:
                    if 'error' in result:
                        failed_articles.append(result['error'])

    # ìµœì¢… ì €ì¥
    articles.sort(key=lambda a: (
        a['date']['year'],
        a['date']['month'],
        a['date']['day'],
        a['article_num']
    ))

    # metadata.json ì €ì¥
    metadata = {
        'collection_name': collection_name,
        'source': 'sillok',
        'crawl_mode': 'file',
        'crawl_date': datetime.now().isoformat(),
        'total_articles': len(articles),
        'articles_with_translation': sum(1 for a in articles if a.get('translation', '').strip()),
        'search_params': {},
        'source_detail': list(set(a['source']['sillok'] for a in articles if a['source']['sillok'])),
        'date_range': {
            'start': f"{articles[0]['date']['reign']} {articles[0]['date']['year']}ë…„" if articles else '',
            'end': f"{articles[-1]['date']['reign']} {articles[-1]['date']['year']}ë…„" if articles else ''
        },
        'preprocessing_done': False
    }

    metadata_file = output_dir / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    # ì§„í–‰ íŒŒì¼ ì‚­ì œ
    if progress_file.exists():
        progress_file.unlink()

    # ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
    if failed_articles:
        failed_file = output_dir / "failed_articles.json"
        with open(failed_file, 'w', encoding='utf-8') as f:
            json.dump(failed_articles, f, ensure_ascii=False, indent=2)
        print(f"\nâš ï¸  {len(failed_articles)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ - failed_articles.json ì°¸ì¡°")

    print(f"\nSaved {len(articles)} articles to {output_dir}")
    print(f"  - raw/: ê¸°ì‚¬ë³„ ê°œë³„ JSON")
    print(f"  - metadata.json: ìˆ˜ì§‘ ë©”íƒ€ì •ë³´")
    if failed_articles:
        print(f"  - failed_articles.json: ì‹¤íŒ¨ ë¡œê·¸ ({len(failed_articles)}ê±´)")

    return output_dir


def main():
    parser = argparse.ArgumentParser(description='ì¡°ì„ ì™•ì¡°ì‹¤ë¡ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('input_file', help='ì…ë ¥ íŒŒì¼ (TSV ë˜ëŠ” JSON)')
    parser.add_argument('--name', '-n', required=True, help='ì»¬ë ‰ì…˜ ì´ë¦„ (ì˜ˆ: ì†¡ì‹œì—´_íš¨ì¢…ì‹¤ë¡)')
    parser.add_argument('--format', '-f', choices=['auto', 'tsv', 'json'], default='auto',
                        help='ì…ë ¥ íŒŒì¼ í˜•ì‹ (ê¸°ë³¸: auto - í™•ì¥ìë¡œ íŒë‹¨)')
    parser.add_argument('--workers', '-w', type=int, default=4, help='ë³‘ë ¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 4)')
    parser.add_argument('--resume', '-r', action='store_true', help='ì¤‘ë‹¨ëœ ì‘ì—… ì¬ê°œ')
    parser.add_argument('--preprocess', '-p', action='store_true',
                        help='í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì•ˆë‚´')

    args = parser.parse_args()

    # ì…ë ¥ íŒŒì¼ íŒŒì‹±
    print(f"Reading {args.input_file}...")
    entries = parse_input_file(args.input_file, input_format=args.format)
    print(f"  Found {len(entries)} entries")

    if not entries:
        print("No entries found in input file")
        sys.exit(1)

    # í¬ë¡¤ë§ ì‹¤í–‰
    output_dir = crawl_articles(
        entries,
        args.name,
        num_workers=args.workers,
        resume=args.resume
    )

    print(f"\nDone! Collection saved to: {output_dir}")

    # ì „ì²˜ë¦¬ ì•ˆë‚´
    if args.preprocess:
        print("\n" + "="*60)
        print("ğŸ“Œ ì „ì²˜ë¦¬ ì‹¤í–‰ ì•ˆë‚´")
        print("="*60)
        print(f"\nClaudeì—ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì²­í•˜ì„¸ìš”:")
        print(f'  "DB/Sillok/{args.name} ì „ì²˜ë¦¬í•´ì¤˜"')
        print("="*60)
    else:
        print("\në‹¤ìŒ ë‹¨ê³„: ì „ì²˜ë¦¬ (ìš”ì•½ + ì¸ë±ì‹±) ì§„í–‰")
        print(f'  Claudeì—ê²Œ "DB/Sillok/{args.name} ì „ì²˜ë¦¬í•´ì¤˜" ë¼ê³  ìš”ì²­í•˜ì„¸ìš”.')


if __name__ == '__main__':
    main()
